# -*- coding: utf-8 -*-
"""asl_fingerspelling

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jMRLBCuiqZd1RY6p0FuqDX5g4gr4Xuto
"""

import glob
import sys

import cv2
import numpy as np
import os
import tensorflow as tf
from handshape_feature_extractor import HandShapeFeatureExtractor
import keras
load_model = keras.models.load_model

# !rm -rf data/*

def get_inference_vector_one_frame_alphabet(files_list):
    # model trained based on https://www.kaggle.com/mrgeislinger/asl-rgb-depth-fingerspelling-spelling-it-out

    model = HandShapeFeatureExtractor.get_instance()
    vectors = []
    video_names = []
    step = int(len(files_list) / 100)
    if step == 0:
        step = 1

    count = 0
    for video_frame in files_list:
        # print(video_frames)
        # assert len(video_frames) == 6

        img = cv2.imread(video_frame)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

        results = model.extract_feature(img)
        results = np.squeeze(results)

        vectors.append(results)
        video_names.append(os.path.basename(video_frame))

        count += 1
        if count % step == 0:
            sys.stdout.write("-")
            sys.stdout.flush()

    return vectors


def load_labels(label_file):
    label = []
    proto_as_ascii_lines = tf.io.gfile.GFile(label_file).readlines()
    for l in proto_as_ascii_lines:
        label.append(l.rstrip())
    return label

def load_label_dicts(label_file):
    id_to_labels = load_labels(label_file)
    labels_to_id = {}
    i = 0

    for id in id_to_labels:
        labels_to_id[id] = i
        i += 1

    return id_to_labels, labels_to_id

files = []
video_folder_path = os.path.join('data')
# wildcard to select all frames for given video file

path = os.path.join(video_folder_path, "*.png")
frames = glob.glob(path)
# sort image frames
frames.sort()
files = frames

label_file = 'output_labels_alphabet.txt'
id_to_labels, labels_to_id = load_label_dicts(label_file)

X_train = []
Y_train = []
for fileName in files:
        img = cv2.imread(fileName)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        img = cv2.resize(img, (200, 200))
        img_arr = np.array(img) / 255.0
        img_arr = img_arr.reshape(200, 200, 1)
        X_train.append(img_arr)
        label = fileName.split('/')[-1].split(' ')[0]
        label_id = labels_to_id[label]
        Y_train.append(label_id)
X_train = np.array(X_train)
Y_train = np.array(Y_train)

model = load_model('cnn_model.h5')
model.fit(X_train, Y_train, epochs = 5)

prediction_vector = get_inference_vector_one_frame_alphabet(files)

len(prediction_vector)

# for p in prediction_vector:
#   print(id_to_labels[max([x for x in range(27)], key = lambda i: p[i])])
